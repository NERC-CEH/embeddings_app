components:
  llm:
    init_parameters:
      generation_kwargs:
        max_new_tokens: 100
        temperature: 0.9
      huggingface_pipeline_kwargs:
        device: cpu
        model: google/flan-t5-large
        task: text2text-generation
      stop_words: null
      streaming_callback: null
      token:
        env_vars:
        - HF_API_TOKEN
        strict: false
        type: env_var
    type: haystack.components.generators.hugging_face_local.HuggingFaceLocalGenerator
  prompt_builder:
    init_parameters:
      required_variables: null
      template: "\n    Given the following information, answer the question.\n\n \
        \   Question: {{query}}\n\n    Context:\n    {% for document in documents\
        \ %}\n        {{ document.content }}\n    {% endfor %}\n\n    Answer:\n  \
        \  "
      variables: null
    type: haystack.components.builders.prompt_builder.PromptBuilder
  retriever:
    init_parameters:
      document_store:
        init_parameters:
          collection_name: eidc_datasets
          embedding_function: default
          persist_path: chroma-data
        type: haystack_integrations.document_stores.chroma.document_store.ChromaDocumentStore
      filters: null
      top_k: 2
    type: haystack_integrations.components.retrievers.chroma.retriever.ChromaQueryTextRetriever
connections:
- receiver: prompt_builder.documents
  sender: retriever.documents
- receiver: llm.prompt
  sender: prompt_builder.prompt
max_loops_allowed: 100
metadata: {}